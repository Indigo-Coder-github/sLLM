{
    "sota_1b_model_kwargs": {
        "Qwen/Qwen3-1.7B": {
            "per_device_train_batch_size": 1,
            "per_device_eval_batch_size": 1,
            "gradient_accumulation_steps": 256,
            "learning_rate": 1e-4,
            "num_train_epochs": 3,
            "save_strategy": "best",
            "eval_steps": 0.2
        },
        "Qwen/Qwen2.5-1.5B-Instruct": {
            "per_device_train_batch_size": 1,
            "per_device_eval_batch_size": 1,
            "gradient_accumulation_steps": 256,
            "learning_rate": 1e-4,
            "num_train_epochs": 3,
            "save_strategy": "best",
            "eval_steps": 0.2
        },
        "naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B": {
            "per_device_train_batch_size": 1,
            "per_device_eval_batch_size": 1,
            "gradient_accumulation_steps": 256,
            "learning_rate": 1e-4,
            "num_train_epochs": 3,
            "save_strategy": "best",
            "eval_steps": 0.2
        },
        "google/gemma-3-1b-it": {
            "per_device_train_batch_size": 1,
            "per_device_eval_batch_size": 1,
            "gradient_accumulation_steps": 256,
            "learning_rate": 1e-4,
            "num_train_epochs": 3,
            "save_strategy": "best",
            "eval_steps": 0.2
        },
        "meta-llama/Llama-3.2-1B-Instruct": {
            "per_device_train_batch_size": 1,
            "per_device_eval_batch_size": 1,
            "gradient_accumulation_steps": 256,
            "learning_rate": 1e-4,
            "num_train_epochs": 3,
            "save_strategy": "best",
            "eval_steps": 0.2
        }
    }
}